# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_CTRPzRQnQYNekJZM98FUZkPEAdFulhz
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import warnings
import nltk
from nltk.corpus import cmudict
nltk.download('cmudict')
d = cmudict.dict()
warnings.filterwarnings("ignore")

file_names = ["StopWords_Auditor.txt", "StopWords_Currencies.txt", "StopWords_DatesandNumbers.txt", "StopWords_Generic.txt", "StopWords_GenericLong.txt", "StopWords_Geographic.txt", "StopWords_Names.txt"]
output_file_name = "combined_stopwords.txt"
with open(output_file_name, "w", encoding="UTF-8") as combined_file:
    for file_name in file_names:
        try:
            with open(file_name, "r", encoding="ISO-8859-1") as input_file:
                file_content = input_file.read()
                combined_file.write(file_content)
                combined_file.write("\n")
        except FileNotFoundError:
            print(f"File {file_name} not found")
print("Files combined successfully!")

stop_words = set()
with open("/content/combined_stopwords.txt", encoding="utf-8") as stop_words_file:
    for line in stop_words_file:
        word = line.strip().split("|")[0].strip()
        stop_words.add(word.lower())
input_data = pd.read_csv("/content/Input.xlsx - Sheet1.csv")
input_data = pd.DataFrame(input_data)
positive_words = set(open("positive-words.txt", encoding="ISO-8859-1").read().splitlines())
negative_words = set(open("negative-words.txt", encoding="ISO-8859-1").read().splitlines())
output_data = pd.DataFrame(columns=["URL_ID", "URL","Positive_Score", "Negative_Score", "Polarity_Score",
                                    "Subjectivity_Score", "Average_Sentence_Length", "Percentage_of_Complex_Words",
                                    "FOG_Index", "Average_Number_of_Words_Per_Sentence", "Complex_Word_Count",
                                    "Word_Count", "Syllable_Per_Word", "Personal_Pronouns", "Average_Word_Length"])

def count_syllables_nltk(word):
    if word.lower() in d:
        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])
    else:
        return 0

for index, row in input_data.iterrows():
    url_id = row["URL_ID"]
    url = row["URL"]
    try:
        response = requests.get(url)
        content = response.content
        soup = BeautifulSoup(content, "html.parser")
        article_text = soup.get_text()
    except Exception as e:
        print(f"Error fetching content from URL {url}: {e}")
        continue
    output_data = output_data.append({"URL": url,}, ignore_index=True)
    positive_score = sum(1 for word in article_text.split() if word in positive_words)
    negative_score = sum(1 for word in article_text.split() if word in negative_words)
    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)
    subjectivity_score = (positive_score + negative_score) / (len(article_text.split()) + 0.000001)
    cleaned_words = [word for word in article_text.split() if word.lower() not in stop_words]
    cleaned_text = " ".join(cleaned_words)
    words = cleaned_text.split()
    total_words = len(words)
    total_sentences = cleaned_text.count('.') + cleaned_text.count('!') + cleaned_text.count('?')
    average_sentence_length = total_words / total_sentences
    complex_word_count = sum(1 for word in words if count_syllables_nltk(word) > 2)
    percentage_complex_words = complex_word_count / total_words
    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)
    average_words_per_sentence = total_words / total_sentences
    total_syllables = sum(count_syllables_nltk(word) for word in words)
    syllables_per_word = total_syllables / total_words
    personal_pronouns = len(re.findall(r'\b(i|we|my|ours|us)\b', cleaned_text, re.IGNORECASE))
    total_characters = sum(len(word) for word in words)
    average_word_length = total_characters / total_words
    output_data = output_data.append({
    "URL_ID": url_id,

    "Positive_Score": "{:.2f}".format(positive_score),
    "Negative_Score": "{:.2f}".format(negative_score),
    "Polarity_Score": "{:.2f}".format(polarity_score),
    "Subjectivity_Score": "{:.2f}".format(subjectivity_score),
    "Average_Sentence_Length": "{:.2f}".format(average_sentence_length),
    "Percentage_of_Complex_Words": "{:.2f}".format(percentage_complex_words),
    "FOG_Index": "{:.2f}".format(fog_index),
    "Average_Number_of_Words_Per_Sentence": "{:.2f}".format(average_words_per_sentence),
    "Complex_Word_Count": complex_word_count,
    "Word_Count": total_words,
    "Syllable_Per_Word": "{:.2f}".format(syllables_per_word),
    "Personal_Pronouns": personal_pronouns,
    "Average_Word_Length": "{:.2f}".format(average_word_length)
}, ignore_index=True)

output_data.to_csv("output.csv", index=False)

